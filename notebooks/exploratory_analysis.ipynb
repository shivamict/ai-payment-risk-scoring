{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AI Payment Risk Scoring - Exploratory Data Analysis\n",
    "\n",
    "This notebook provides comprehensive exploratory data analysis for the AI-based customer payment risk evaluation system.\n",
    "\n",
    "## Overview\n",
    "- **Objective**: Analyze customer payment data to understand risk patterns\n",
    "- **Data**: Customer demographics, financial history, and payment behavior\n",
    "- **Target**: Payment failure prediction and risk scoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Configure plotting\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', None)\n",
    "\n",
    "print(\"Libraries imported successfully!\")\n",
    "\n",
    "# Load the data\n",
    "data_path = '../data/raw/your_excel_file.xlsx'  # Update with your actual file path\n",
    "df = pd.read_excel(data_path)\n",
    "\n",
    "# Display the first few rows of the dataset\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import custom modules\n",
    "import sys\n",
    "import os\n",
    "sys.path.append(os.path.join(os.getcwd(), '..', 'src'))\n",
    "\n",
    "from data_preparation import DataPreparator\n",
    "from model_training import ModelTrainer\n",
    "from scoring import RiskScorer\n",
    "from utils import ResultsExporter\n",
    "import config\n",
    "\n",
    "print(\"Custom modules imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data Generation and Loading\n",
    "\n",
    "Let's start by generating sample data or loading existing data for analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Check for missing values\n",
    "missing_values = df.isnull().sum()\n",
    "missing_values[missing_values > 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Overview and Quality Assessment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Visualize the distribution of the target variable\n",
    "plt.figure(figsize=(8, 5))\n",
    "sns.countplot(x='未払FLAG', data=df)\n",
    "plt.title('Distribution of Payment Default Flag')\n",
    "plt.xlabel('未払FLAG')\n",
    "plt.ylabel('Count')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Initialize data preparator\n",
    "data_prep = DataPreparator()\n",
    "\n",
    "# Generate sample data\n",
    "print(\"Generating sample customer data...\")\n",
    "sample_data = data_prep.generate_sample_data(n_customers=1000, n_transactions_per_customer=5)\n",
    "\n",
    "print(f\"Generated data shape: {sample_data.shape}\")\n",
    "print(f\"Columns: {list(sample_data.columns)}\")\n",
    "sample_data.head()\n",
    "\n",
    "# Visualize correlations between features\n",
    "plt.figure(figsize=(12, 8))\n",
    "correlation_matrix = df.corr()\n",
    "sns.heatmap(correlation_matrix, annot=True, fmt='.2f', cmap='coolwarm')\n",
    "plt.title('Correlation Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Customer-Level Data Analysis\n",
    "\n",
    "Let's aggregate the transaction data to customer level and analyze patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregate to customer level\n",
    "customer_data = data_prep.aggregate_customer_data(sample_data)\n",
    "\n",
    "print(f\"Customer-level data shape: {customer_data.shape}\")\n",
    "print(f\"Unique customers: {len(customer_data)}\")\n",
    "customer_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Customer demographics analysis\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "\n",
    "# Age distribution\n",
    "customer_data['age'].hist(bins=30, ax=axes[0, 0], alpha=0.7)\n",
    "axes[0, 0].set_title('Age Distribution')\n",
    "axes[0, 0].set_xlabel('Age')\n",
    "axes[0, 0].set_ylabel('Frequency')\n",
    "\n",
    "# Income distribution\n",
    "customer_data['income'].hist(bins=30, ax=axes[0, 1], alpha=0.7, color='green')\n",
    "axes[0, 1].set_title('Income Distribution')\n",
    "axes[0, 1].set_xlabel('Income')\n",
    "axes[0, 1].set_ylabel('Frequency')\n",
    "\n",
    "# Credit score distribution\n",
    "customer_data['credit_score'].hist(bins=30, ax=axes[0, 2], alpha=0.7, color='orange')\n",
    "axes[0, 2].set_title('Credit Score Distribution')\n",
    "axes[0, 2].set_xlabel('Credit Score')\n",
    "axes[0, 2].set_ylabel('Frequency')\n",
    "\n",
    "# Account balance distribution\n",
    "customer_data['account_balance'].hist(bins=30, ax=axes[1, 0], alpha=0.7, color='red')\n",
    "axes[1, 0].set_title('Account Balance Distribution')\n",
    "axes[1, 0].set_xlabel('Account Balance')\n",
    "axes[1, 0].set_ylabel('Frequency')\n",
    "\n",
    "# Total transactions\n",
    "customer_data['total_transactions'].hist(bins=20, ax=axes[1, 1], alpha=0.7, color='purple')\n",
    "axes[1, 1].set_title('Total Transactions Distribution')\n",
    "axes[1, 1].set_xlabel('Number of Transactions')\n",
    "axes[1, 1].set_ylabel('Frequency')\n",
    "\n",
    "# Payment failure rate\n",
    "if 'payment_failure' in customer_data.columns:\n",
    "    failure_counts = customer_data['payment_failure'].value_counts()\n",
    "    axes[1, 2].pie(failure_counts.values, labels=['No Failure', 'Failure'], autopct='%1.1f%%')\n",
    "    axes[1, 2].set_title('Payment Failure Distribution')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusioninformation\n",
    "print(\"=== DATA OVERVIEW ===\")\n",
    "This exploratory data analysis reveals:\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "The analysis confirms the dataset is suitable for training an effective payment risk scoring model.5. **Model Readiness**: Data is well-prepared for machine learning pipeline4. **Feature Relationships**: Clear patterns between financial metrics and payment risk3. **Target Balance**: Reasonable class distribution for payment failures2. **Feature Distribution**: Well-distributed customer demographics and financial metrics1. **Data Quality**: Clean dataset with minimal missing values\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "This exploratory data analysis provides insights into the dataset, including the distribution of the target variable and correlations between features. Further steps will involve data preparation and model training.## Conclusionsample_data.describe()print(\"\\n=== BASIC STATISTICS ===\")\n",
    "print(missing_values[missing_values > 0])missing_values = sample_data.isnull().sum()\n",
    "\n",
    "print(\"\\n=== MISSING VALUES ===\")print(sample_data.dtypes)print(\"\\n=== DATA TYPES ===\")print(f\"Memory usage: {sample_data.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "plt.show()plt.tight_layout()axes[1, 1].set_xlabel('Bytes')axes[1, 1].set_title('Top 10 Columns by Memory Usage')axes[1, 1].barh(memory_usage.index, memory_usage.values)\n",
    "\n",
    "\n",
    "memory_usage = sample_data.memory_usage(deep=True).sort_values(ascending=False)[:10]\n",
    "\n",
    "# Memory usage by columnaxes[1, 0].set_title('Duplicate Records')axes[1, 0].bar(['Unique', 'Duplicate'], [unique_count, duplicate_count])\n",
    "\n",
    "unique_count = len(sample_data) - duplicate_countduplicate_count = sample_data.duplicated().sum()# Data quality visualization\n",
    "\n",
    "\n",
    "\n",
    "# Duplicate recordsaxes[0, 1].set_title('Data Types Distribution')fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "\n",
    "axes[0, 1].pie(dtype_counts.values, labels=dtype_counts.index, autopct='%1.1f%%')\n",
    "# Missing values heatmap\n",
    "sns.heatmap(sample_data.isnull(), cbar=True, ax=axes[0, 0], cmap='viridis')\n",
    "axes[0, 0].set_title('Missing Values Heatmap')\n",
    "\n",
    "# Data types distribution\n",
    "dtype_counts = sample_data.dtypes.value_counts()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
